{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Problem Statement\nUTKFace Dataset is a large-scale face dataset withalong age span (range from 0 to 116 years old). The objective is to classify each face based  on  gender  using  CNNs  on  Tensorflow  2.x  and  then  use OpenCV & Haar Cascade File to check the gender in real-time","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.models import Sequential","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-02T11:30:54.970472Z","iopub.execute_input":"2021-11-02T11:30:54.970819Z","iopub.status.idle":"2021-11-02T11:30:57.149428Z","shell.execute_reply.started":"2021-11-02T11:30:54.970755Z","shell.execute_reply":"2021-11-02T11:30:57.148586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration:Analyze and explore the dataset","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/utkface-new/UTKFace/\"\nfiles = os.listdir(path)\nsize = len(files)\nprint(\"Total samples:\",size)\nprint(files[0])","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-11-02T11:31:03.13978Z","iopub.execute_input":"2021-11-02T11:31:03.140117Z","iopub.status.idle":"2021-11-02T11:31:03.580783Z","shell.execute_reply.started":"2021-11-02T11:31:03.140064Z","shell.execute_reply":"2021-11-02T11:31:03.579981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of features\",len(files[0].split(\"_\"))-1)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:31:05.941289Z","iopub.execute_input":"2021-11-02T11:31:05.941596Z","iopub.status.idle":"2021-11-02T11:31:05.947373Z","shell.execute_reply.started":"2021-11-02T11:31:05.941539Z","shell.execute_reply":"2021-11-02T11:31:05.946427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The shape of the dataset is(23708,3)\n### where the number of images are 23708 and the features are 3 i.e., age, gender and race. Since we are doing gender detection we will only keep that feature","metadata":{}},{"cell_type":"code","source":"images = []\nages = []\ngenders = []\nfor file in files:\n    image = cv2.imread(path+'/'+file,1)\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image,dsize=(64,64))\n    image = image.reshape((image.shape[0],image.shape[1],3))\n    images.append(image)\n    split_var = file.split('_')\n    genders.append(int(split_var[1]) )","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:31:12.488307Z","iopub.execute_input":"2021-11-02T11:31:12.488642Z","iopub.status.idle":"2021-11-02T11:32:53.593735Z","shell.execute_reply.started":"2021-11-02T11:31:12.488583Z","shell.execute_reply":"2021-11-02T11:32:53.592991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image = cv2.imread(path+'/'+files[0],1)\n# image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.imshow(images[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:33:47.282022Z","iopub.execute_input":"2021-11-02T11:33:47.282314Z","iopub.status.idle":"2021-11-02T11:33:47.457752Z","shell.execute_reply.started":"2021-11-02T11:33:47.282263Z","shell.execute_reply":"2021-11-02T11:33:47.457008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_gender = list(set(genders))\ny_gender = [genders.count(i) for i in x_gender]\nplt.bar(x_gender,y_gender)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:34:45.072434Z","iopub.execute_input":"2021-11-02T11:34:45.072772Z","iopub.status.idle":"2021-11-02T11:34:45.206056Z","shell.execute_reply.started":"2021-11-02T11:34:45.072716Z","shell.execute_reply":"2021-11-02T11:34:45.205417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unique count of male and female","metadata":{}},{"cell_type":"code","source":"print(y_gender)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:34:50.183118Z","iopub.execute_input":"2021-11-02T11:34:50.183416Z","iopub.status.idle":"2021-11-02T11:34:50.191292Z","shell.execute_reply.started":"2021-11-02T11:34:50.183362Z","shell.execute_reply":"2021-11-02T11:34:50.190601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the images present in the dataset","metadata":{}},{"cell_type":"code","source":"def display(img):\n    plt.imshow(img)\nidx = 2010\nsample = images[idx]\nprint(\"Gender:\",genders[idx])\ndisplay(sample)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:34:36.407973Z","iopub.execute_input":"2021-11-02T11:34:36.408269Z","iopub.status.idle":"2021-11-02T11:34:36.551962Z","shell.execute_reply.started":"2021-11-02T11:34:36.408216Z","shell.execute_reply":"2021-11-02T11:34:36.55115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the dataset for the model","metadata":{}},{"cell_type":"markdown","source":"### Normalize the data ","metadata":{}},{"cell_type":"code","source":"# pre processing\ntarget = np.zeros((size),dtype='float32')\nfeatures = np.zeros((size,sample.shape[0],sample.shape[1],3),dtype = 'float32')\nfor i in range(size):\n    target[i] = int(genders[i])\n    features[i] = images[i]\nfeatures = features / 255\ndisplay(features[552])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:25.084783Z","iopub.execute_input":"2021-11-02T11:35:25.085125Z","iopub.status.idle":"2021-11-02T11:35:26.778443Z","shell.execute_reply.started":"2021-11-02T11:35:25.085069Z","shell.execute_reply":"2021-11-02T11:35:26.777764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target[552]","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:17.890579Z","iopub.execute_input":"2021-11-02T11:35:17.890886Z","iopub.status.idle":"2021-11-02T11:35:17.897876Z","shell.execute_reply.started":"2021-11-02T11:35:17.890832Z","shell.execute_reply":"2021-11-02T11:35:17.895894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding the labels: encoding is not required since they are in binary format","metadata":{}},{"cell_type":"markdown","source":"## Split the dataset using sklearnâ€™s train_test_split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2,shuffle  = True)\nprint(\"Samples in Training:\",x_train.shape[0])\nprint(\"Samples in Testing:\",x_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:33.023014Z","iopub.execute_input":"2021-11-02T11:35:33.023314Z","iopub.status.idle":"2021-11-02T11:35:34.500149Z","shell.execute_reply.started":"2021-11-02T11:35:33.02326Z","shell.execute_reply":"2021-11-02T11:35:34.499329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:36.553236Z","iopub.execute_input":"2021-11-02T11:35:36.553583Z","iopub.status.idle":"2021-11-02T11:35:36.561471Z","shell.execute_reply.started":"2021-11-02T11:35:36.553517Z","shell.execute_reply":"2021-11-02T11:35:36.560354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:24:39.949522Z","iopub.execute_input":"2021-10-31T06:24:39.94983Z","iopub.status.idle":"2021-10-31T06:24:40.09181Z","shell.execute_reply.started":"2021-10-31T06:24:39.949776Z","shell.execute_reply":"2021-10-31T06:24:40.090898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:22:14.628966Z","iopub.execute_input":"2021-10-31T06:22:14.629265Z","iopub.status.idle":"2021-10-31T06:22:16.232844Z","shell.execute_reply.started":"2021-10-31T06:22:14.629216Z","shell.execute_reply":"2021-10-31T06:22:16.232105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Develop the model for recognizing the gender","metadata":{}},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(64,64,3)))\nmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, kernel_size=(3, 3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:43.982268Z","iopub.execute_input":"2021-11-02T11:35:43.982623Z","iopub.status.idle":"2021-11-02T11:35:46.587904Z","shell.execute_reply.started":"2021-11-02T11:35:43.982566Z","shell.execute_reply":"2021-11-02T11:35:46.587071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:49.120557Z","iopub.execute_input":"2021-11-02T11:35:49.120871Z","iopub.status.idle":"2021-11-02T11:35:49.133102Z","shell.execute_reply.started":"2021-11-02T11:35:49.120818Z","shell.execute_reply":"2021-11-02T11:35:49.131853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = 'adam', loss =['binary_crossentropy'],metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:54.90079Z","iopub.execute_input":"2021-11-02T11:35:54.901093Z","iopub.status.idle":"2021-11-02T11:35:54.952945Z","shell.execute_reply.started":"2021-11-02T11:35:54.901039Z","shell.execute_reply":"2021-11-02T11:35:54.952278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:23:26.003243Z","iopub.execute_input":"2021-10-31T06:23:26.003567Z","iopub.status.idle":"2021-10-31T06:23:26.016448Z","shell.execute_reply.started":"2021-10-31T06:23:26.003513Z","shell.execute_reply":"2021-10-31T06:23:26.015586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs = 30, batch_size=64,shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:35:58.33441Z","iopub.execute_input":"2021-11-02T11:35:58.33476Z","iopub.status.idle":"2021-11-02T11:38:35.288971Z","shell.execute_reply.started":"2021-11-02T11:35:58.334701Z","shell.execute_reply":"2021-11-02T11:38:35.288185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot accuracy curves during training and validation","metadata":{}},{"cell_type":"code","source":"plt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:38:47.158118Z","iopub.execute_input":"2021-11-02T11:38:47.158432Z","iopub.status.idle":"2021-11-02T11:38:47.313525Z","shell.execute_reply.started":"2021-11-02T11:38:47.158376Z","shell.execute_reply":"2021-11-02T11:38:47.312628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot loss curves","metadata":{}},{"cell_type":"code","source":"plt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:38:54.043771Z","iopub.execute_input":"2021-11-02T11:38:54.044074Z","iopub.status.idle":"2021-11-02T11:38:54.180305Z","shell.execute_reply.started":"2021-11-02T11:38:54.044024Z","shell.execute_reply":"2021-11-02T11:38:54.179665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the model file","metadata":{}},{"cell_type":"code","source":"model.save('model_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:38:59.950037Z","iopub.execute_input":"2021-11-02T11:38:59.95034Z","iopub.status.idle":"2021-11-02T11:39:00.093202Z","shell.execute_reply.started":"2021-11-02T11:38:59.950284Z","shell.execute_reply":"2021-11-02T11:39:00.09238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-10-31T06:26:54.181451Z","iopub.execute_input":"2021-10-31T06:26:54.181791Z","iopub.status.idle":"2021-10-31T06:26:54.348932Z","shell.execute_reply.started":"2021-10-31T06:26:54.181736Z","shell.execute_reply":"2021-10-31T06:26:54.348102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model","metadata":{}},{"cell_type":"code","source":"model.evaluate(x_test,y_test,verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:39:04.661947Z","iopub.execute_input":"2021-11-02T11:39:04.662255Z","iopub.status.idle":"2021-11-02T11:39:05.393134Z","shell.execute_reply.started":"2021-11-02T11:39:04.662198Z","shell.execute_reply":"2021-11-02T11:39:05.392462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert the model file to json format ","metadata":{}},{"cell_type":"code","source":"json_config = model.to_json()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:39:10.647253Z","iopub.execute_input":"2021-11-02T11:39:10.64758Z","iopub.status.idle":"2021-11-02T11:39:10.656466Z","shell.execute_reply.started":"2021-11-02T11:39:10.647522Z","shell.execute_reply":"2021-11-02T11:39:10.655469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = model_from_json(json_config)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:39:12.622592Z","iopub.execute_input":"2021-11-02T11:39:12.622885Z","iopub.status.idle":"2021-11-02T11:39:12.86413Z","shell.execute_reply.started":"2021-11-02T11:39:12.622836Z","shell.execute_reply":"2021-11-02T11:39:12.863402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##model.layers[0].weights","metadata":{"execution":{"iopub.status.busy":"2021-11-02T10:41:49.92404Z","iopub.execute_input":"2021-11-02T10:41:49.924381Z","iopub.status.idle":"2021-11-02T10:41:49.928027Z","shell.execute_reply.started":"2021-11-02T10:41:49.924326Z","shell.execute_reply":"2021-11-02T10:41:49.927088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict the Gender of the uploaded image","metadata":{}},{"cell_type":"code","source":"def display(img):\n    plt.imshow(img)\n    \ndef get_gender(prob):\n    if prob < 0.5:return \"Male\"\n    else: return \"Female\"\n\ndef get_result(sample):\n    sample = sample/255\n    val = model.predict( np.array([ sample ]) )    \n    gender = get_gender(val[0])\n    print(\"Values:\",val,\"\\nPredicted Gender:\",gender)\n    \n    \nindexes = [500]\nfor idx in indexes:\n    sample = images[idx]\n    display(sample)\n    print(\"Actual Gender:\",get_gender(genders[idx]))\n    res = get_result(sample)","metadata":{"execution":{"iopub.status.busy":"2021-11-02T11:39:54.762975Z","iopub.execute_input":"2021-11-02T11:39:54.763271Z","iopub.status.idle":"2021-11-02T11:39:55.03264Z","shell.execute_reply.started":"2021-11-02T11:39:54.763218Z","shell.execute_reply":"2021-11-02T11:39:55.031906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Haar cascade to check in real time","metadata":{}},{"cell_type":"code","source":"face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n\ncap=cv2.VideoCapture(0)\n\nwhile True:\n    ret,test_img=cap.read()# captures frame and returns boolean value and captured image\n    \n    if not ret:\n        continue\n    img= cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n    \n    #window for each face detected in the frame\n    \n    faces_detected = face_haar_cascade.detectMultiScale(img, 1.32, 5)\n\n\n    for (x,y,w,h) in faces_detected:\n        \n        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n        \n        roi=img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image\n        \n        roi=cv2.resize(roi,(64,64))\n        \n        img_pixels = image.img_to_array(roi)\n        img_pixels = np.expand_dims(img_pixels, axis = 0)\n        img_pixels /= 255\n\n        predictions = model.predict(img_pixels)\n\n        #find max indexed array\n        max_index = get_gender(predictions[0])\n\n        gender = ('male', 'female')\n        predicted_gender = gender[max_index]\n\n        cv2.putText(test_img, predicted_gender, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n\n    resized_img = cv2.resize(test_img, (1000, 700))\n    cv2.imshow('Facial emotion analysis ',resized_img)\n\n\n\n    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed\n        break\n\ncap.release()\ncv2.destroyAllWindows","metadata":{},"execution_count":null,"outputs":[]}]}